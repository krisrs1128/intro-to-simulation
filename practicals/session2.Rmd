---
title: "Multivariate Simulation Exercises"
author: Kris Sankaran
output: rmdformats::readthedown
---

<script>
function myFunction(id) {
    var x = document.getElementById(id);
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>
<style>
div .info {
  margin: auto;
  background-color: #EAF0FB;
  width: 95%;
  padding: 10px;
}
</style>

How can we choose sample sizes in more complex bioinformatic workflows, where we
simultaneously analyze many features (taxa, genes, metaabolites) in concert?
While traditional, analytical power analysis often breaks down, simulation can
still be effective. We'll look at a concrete case study where we try to choose a
good sample size for a sparse partial least squares discriminant analysis
(sPLS-DA) of the Type I Diabetes (T1D) gut microbiome.

```{r, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
```

First, we'll load the required packages. Relative to our first session, the only
additional package that we need is `mixOmics`. This can be installed using
`BiocManager::install('mixOmics')`.

```{r}
suppressPackageStartupMessages({
  library(MIGsim)
  library(SummarizedExperiment)
  library(dplyr)
  library(gamboostLSS)
  library(glue)
  library(mixOmics)
  library(ggplot2)
  library(scDesigner)
})
set.seed(202406)
theme_set(theme_classic())
```

# Interpreting PLS-DA

The T1D dataset below describes 427 metabolites from the gut microbiomes of 40
T1D patients and 61 healthy controls. These data were originally gathered by
[Gavin et al. (2018)](https://doi.org/10.2337/dc18-0777), who were motivated by
the relationship between the pancreas and intestinal issues often experienced by
T1D patients. They were especially curious about whether microbime-associated
proteins might be related to increased risk for T1D development.

```{r}
data(t1d)
```

The output that we care about are the PLD-DA scores and loadings. The wrapper
below directly gives us this output without our having to explicitly set
hyerparameters, though you can [look here](https://github.com/krisrs1128/intro-to-simulation/blob/main/MIGsim/R/methods.R) 
to see how the function was defined.

```{r}
result <- splsda_fit(t1d)
plotIndiv(result$fit)
plotVar(result$fit, cutoff = 0.4, cex = 4)
```

**Exercise**: Discuss the output of `plotIndiv`. How does `plotVar` shape
your interpretation?
<button onclick="myFunction(&#39;q0&#39;)">
Show solution
</button>

::: {#q0 style="display:none"}
The first two dimensions of the sPLS-DA pick on distinctions between the T1D
(orange) and control (blue) groups. The variance by these first two dimensions
is relatively small
-- variation in protein signatures may not be easiliy captured in two dimensions
or may not generally be correlated with the disease response. Since most of the
differences between groups are captured by the first dimension, we can look for
features that are highly correlated with this dimension to see which proteins
might be driving the difference. For example, we can conclude that T1D samples
tend to have higher levels of
[p21333](https://www.uniprot.org/uniprotkb/P21333/entry) and
[q8wz42](https://www.uniprot.org/uniprotkb/Q8WZ42/entry) compared to the
controls. 
:::

# Estimation

Let's estimate a simulator where every protein is allowed ot vary across T1D
type. Since the data have already been centered log-ratio transformed, it's okay
to treat these as Gaussian. 

```{r}
simulator <- setup_simulator(t1d, link_formula = ~outcome2, family = ~ GaussianLSS()) |>
  estimate(mstop = 100)
```

# Evaluation

Last time, we saw how we could visualize marginal simulator quality. How can we
tell whether a joint simulator is working, though? One simple check is to
analyze the pairwise correlations. Since the copula model is designed to capture
second-order moments, it should at the very least effectively capture the
correlations.

We've written a small helper that visualizes the pairwise protein-protein
correlations from both the real and the simulated datasets. We seem to be often
overestimating the correlation strength. This is likely a consequence of the
high-dimensionality of the problem.

```{r}
sim_exper <- sample(simulator)
correlation_hist(t1d, sim_exper)
```

**Exercise**: To address this, let's try modifying the `copula_def` argument of
`setup_simulator` to use a more suitable simulator. Generate new correlation
histograms and comment on the changes you observe. You only need to modify the
commented lines (`#`) lines in the block below.

```{r, eval = FALSE}
simulator <- setup_simulator(
  t1d,
  link_formula = ~outcome2,
  family = ~ GaussianLSS(),
  copula_def = # fill this code in
  ) |>
  estimate(mstop = 100)

sim_exper <- sample(simulator) # and then run these two lines
correlation_hist(t1d, sim_exper) #
```

<button onclick="myFunction(&#39;q1&#39;)">
Show solution
</button>

::: {#q1 style="display:none"}
For our coupla, we can use a covariance estimator of [Cai and Liu
(2011)](https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10560), that is
suited for high dimensions. Larger values of `thr` will increase the stability
of our estimates, but at the cost of potentially missing or weakening true
correlations. In line with this point, our new simulated correlations are more
concentrated.
```{r}
simulator <- setup_simulator(
  t1d,
  link_formula = ~outcome2,
  family = ~ GaussianLSS(),
  copula_def = copula_adaptive(thr = 0.2)
) |>
  estimate(mstop = 100)

sim_exper <- sample(simulator)
correlation_hist(t1d, sim_exper)
```
:::

# PLS-DA Power Analysis

Now that we have a simulator, we can run a power analysis. In theory, we could
look at how any summary from the PLS-DA output varies as the sample size
increases. The most natural one, though, is simply to see how classifier
performance improves as we gather more samples. Specifically, we'll measure the
holdout Area Under the Curve (auc), a measure of how well the trains PLS-DA
classifier balance precision and recall on new samples.

Moreover, we'll study the effect of sparsity -- what happens when many features
have no relationship at all with the response? We'll also simulate three
hypothetical datasets for each sample size and sparsity level. All
configurations of interest are stored in the `config` matrix below.

```{r, power-analysis-params}
config <- expand.grid(
  sample_size = floor(seq(15, 150, length.out = 5)),
  n_rep = 1:3,
  n_null = floor(seq(317, 417, length.out = 4)),
  metrics = NA
)

data(t1d_order)
```

**Exercise**: Finally, we're in a position to generate synthetic data and
evaluate PLS-DA performance. Fill in the block below to update the simulator for
each `i`. Remember that the original `simulator` defined above assumes that all
proteins are associated with T1D. You can use `t1d_order` to prioritize the
proteins with the strongest effects in the original data. As before, you should
only need to modify the line marked with the comments (`#`).

```{r power-analysis-loop-setup, eval = FALSE}
for (i in seq_len(nrow(config))) {
  simulator <- simulator |>
    mutate(
      # fill this in
    ) |>
    estimate(mstop = 100)

  config$metrics[i] <- (sample_n(simulator, config$sample_size[i]) |>
    splsda_fit())[["auc"]]
  print(glue("run {i}/{nrow(config)}"))
}
```

<button onclick="myFunction(&#39;power_loop&#39;)">
Show solution
</button>
::: {#power_loop style="display:none"}
To define nulls, we mutate the weakest proteins so that there is no longer any
association with T1D: `link = ~ 1` instead of `link = ~ outcome2`. To speed up
the computation, we organized the `mutate` calls so that we don't need to
re-estimate proteins whose effects were removed in a previous iteration.
```{r power-analysis-loop-soln}
for (i in seq_len(nrow(config))) {
  if (i == 1 || config$n_null[i] != config$n_null[i - 1]) {
    simulator <- simulator |>
      mutate(any_of(rev(t1d_order)[1:config$n_null[i]]), link = ~1) |>
      estimate(mstop = 100)
  }

  config$metrics[i] <- (sample_n(simulator, config$sample_size[i]) |>
    splsda_fit())[["auc"]]
  print(glue("run {i}/{nrow(config)}"))
}
```
:::

We can visualize variation in performance. 

```{r}
ggplot(config, aes(sample_size, metrics, col = factor(n_null))) +
  geom_point() +
  facet_wrap(~n_null)
```


**Discussion**: Interpret the visualization above. How do you think analysis
like this could help you justify making some experimental investments over
others?

<button onclick="myFunction(&#39;discussion&#39;)">
Show solution
</button>

::: {#discussion style="display:none"}
Reading across facets from top left to bottom right, power decreases when the
number of null proteins increases. It seems that sPLS-DA can benefit from having
many weakly associated features. While power is sometimes high in low sample
sizes, the variance can be quite large. In all settings, there is a noticeable
decrease in variance in power as we go from 15 to 48 samples. If we can assume
that a moderate fraction (> 15\%) of measured proteins are associated with T1D,
then we may already achieve good power with ~ 100 samples. However, if we
imagine our effect might be sparser in our future experiment, then this figure
would give us good justification for arguing for a larger number of samples, in
order to ensure we can discover a disease-associated proteomics signature.
:::
