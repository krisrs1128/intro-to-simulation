---
title: "Integrative Simulation Exercises"
author: Kris Sankaran
output: rmdformats::readthedown
---

<script>
function myFunction(id) {
    var x = document.getElementById(id);
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>
<style>
div .info {
  margin: auto;
  background-color: #EAF0FB;
  width: 95%;
  padding: 10px;
}
</style>

Integration can be a subtle exercise. We need to balance our interest in seeing
similarities between datasets with the risk of making things seem more similar
than they really are. Simulation can help navigate this subtlety by letting us
see how integration methods would behave in situations where we know exactly how
the different datasets are related. This note will illustrate this perspective
by showing how simulation can help with both horizontal (across batches) and
vertical (across assays) integration. We'll have a brief interlude on the `map`
function in the `purrr`, which is helpful for concisely writing code that would
otherwise need for loops (e.g., over batches or assays).

As usual, let's load the libraries we'll need. Remember that instructions for
installing `MIGsim` and `scDesigner` are documented in the repository
[README](https://github.com/krisrs1128/intro-to-simulation/).

```{r}
suppressPackageStartupMessages({
  library(MIGsim)
  library(SummarizedExperiment)
  library(gamboostLSS)
  library(ggplot2)
  library(glue)
  library(mixOmics)
  library(purrr)
  library(scDesigner)
})
set.seed(20240603)
```

# Horizontal Integration

The first example is about simultaneously analyzing several batches in a dataset
about the role of the microbiome in Alzheimer's disease. The essential problem
is that, in a typical high-throughput sequencing study, it's not possible to
process all the samples simultaneously, so they are run in separate batches.
Small differences across these runs can lead to systematic differences in the
resulting data, which can obfuscate the more interesting between-group variation
that we intended for the experiment to uncover. For example, in the Alzheimer's
dataset, the date of the sequencing run has a global effect on measured
community composition, which we can see right away from a principal components
plot:

```{r}
data(alzheimers)
pca_batch(alzheimers)
```

You can learn more about the general microbiome batch effect integration problem
in [(Wang and Le Cao, 2020)](https://doi.org/10.1093/bib/bbz105), which is where
this dataset example and the batch effect correction code below comes from. The
article also reviews many more methods for removing these effects and discusses
the situations within which they are most appropriate.

In batch effect correction, it's important to remove as much of the batch
variation as possible without accidentally also removing the real biological
variation that would have been present even if all the samples had been
sequenced together. This is sometimes called ``overintegration,'' and this is an
especially high risk if some of the real biological variation is quite subtle,
e.g., a rare cell type or one that is very similar to a more prominent one.
Simulation can help us gauge the extent to which different methods may or may
not overintegrate. Since we get to control the between-batch and and
between-biological-condition differences, we can see the extent to which
integration methods can remove the former while preserving the latter.

The block below estimates a candidate simulator. By using the formula `~ batch +
treatment`, we're allowing for taxon-wise differences due to batch and
treatment. Note that in principle, we could estimate an interaction between
batch and treatment (the treatment could appear stronger in some batches than
others). I encourage you to try estimating that model; however, visually
analyzing the output suggests that this full model has a tendancy to overfit.
Since the data have already been centered log-ratio transformed, we can try out
a Gaussian marginal model. The Alzheimer's dataset has relatively few samples
compared to the number of features, so we'll use a copula that's designed for
this setting.

```{r}
simulator <- setup_simulator(
  alzheimers,
  ~ batch + treatment,
  ~ GaussianLSS(),
  copula = copula_adaptive(thr = .1)
) |>
  estimate(nu = 0.05, mstop = 100) # lower nu -> stable training
```

We can simulate from the fitted model and evaluate the quality of our fit using
`contrast_boxplot`. This is a light wrapper of the ggplot2 code we used to
compare experiments from our first session, and you can read its definition
[here](https://github.com/krisrs1128/intro-to-simulation/blob/10fa498aea952684204b2f15c387a7983c30626d/MIGsim/R/plot.R#L26).

```{r}
alzheimers_sim <- sample(simulator)
contrast_boxplot(alzheimers, alzheimers_sim)
```

**Exercise** Propose and create at least one other visualization that can be
used to compare and contrast the simulator with real data. What conclusions can
you draw?

<button onclick="myFunction(&#39;q1&#39;)">
Show solution
</button>
::: {#q-vis style="display:none"}
There are many possible answers:
* Boxplots across taxa with different overall abundance levels.
* Analogous histograms or CDF plots, to show the entire distributions, rather than just summarized quantiles.
* Pair scatterplots, to see how well the bivariate relationships between taxa are preserved.
* Dimensionality reduction on the simulated data, to see how well it matches global structure in the original data.

We'll implement the last idea using PCA. This should be contrasted with the PCA
plot on the original data above. It's okay if the plot seems rotated relative to
the oiginal plot -- PCA is only unique up to rotation. The main characteristic
we're looking for is that the relative sizes of the batch and treatment effects
seem reasonaly well-preserved, since these will be the types of effects that our
later batch effect integration methods must be able to distinguish.

```{r}
pca_batch(alzheimers_sim)
```
:::

To study the risk for overintegration, let's imagine that there were a third
treatment group with relatively fewer samples. This is the type of group that a
correction method might accidentally blend in with the rest, if it's too
aggressive. We've defined the imaginary experiment using the data.frame below.
The `treatment` level `1.8` is the new one. We've supposed there are between 1 -
3 technical replicates (`extraction`) for each biological sample (`sample`), and
the batch dates are the same as before.

```{r}
data(imaginary_design)
summary(imaginary_design)
```

We can simulate from the new design and look at how different this new treatment
group seems from the others. It's a subtle effect, definitely smaller than the
batch effect, but also separate enough that we should be able to preserve it.

```{r}
alzheimers_sim <- sample(simulator, new_data = imaginary_design)
pca_batch(alzheimers_sim)
```

We've defined a `batch_correct` wrapper function that implements either the
RUV-III or ComBat batch effect correction methods. Their outputs are contrasted
in the PCAs below. It looks like ComBat might be somewhat too aggressive,
causing the `1` and `1.8` treatment groups to substantially overlap, while RUV
is a bit more conservative, keeping the treatment groups nicely separate. As an
aside, we note that this conclusion can depend on the number of replicates and
total number of samples available. We've included the code for generating the
`imaginary_design` data.frame in [a vignette](https://github.com/krisrs1128/intro-to-simulation/blob/10fa498aea952684204b2f15c387a7983c30626d/MIGsim/vignettes/process-integration.Rmd#L76)
for the `MIGsim` package. Can you find settings that lead either method astray?
(we've found that larger treatment effects can cause RUV to underintegrate).

```{r}
pca_batch(batch_correct(alzheimers_sim, "ruv"))
pca_batch(batch_correct(alzheimers_sim, "combat"))
```

# Interlude: Using map

In the examples below, we'll find it helpful to use the function `map` in the
purrr package. This function gives a one-line replacement for simple for-loops;
it is analogous to list comprehensions in python. It can be useful many places
besides the topic of this tutorial. For example, if we want to convert the
vector `c(1, 2, 3)` into `c(1, 4, 9)`, we can use this map:
```{r}
map(1:3, ~ . ^ 2)
```
The `~` notation is shorthand for defining a function, and the `.` represents
the current vector element.  More generally, we can apply map to lists. This
line will update the list so that 1 is added to each element.
```{r}
map(list(a = 1, b = 2, c = 3), ~ . + 1)
```

To test your understanding, can you write a map that computes the mean for each
vector in the list `x` below? What about the mean of the 10 smallest elements?
```{r}
x <- list(a = rnorm(100), b = rnorm(100, 1))
```
<button onclick="myFunction(&#39;q1&#39;)">
Show solution
</button>
::: {#q-map style="display:none"}
```{r}
purrr::map(x, mean)
purrr::map(x, ~ mean(sort(.)[1:10]))
```
:::

# Vertical Integration - Influence of Nulls

```{r}
data(icu)
simulator <- map(
  icu,
  ~ setup_simulator(., ~ Category, ~ GaussianLSS()) |>
    estimate(nu = 0.05)
)
x <- sample(simulator[[1]])
```

An example where we either do or do not have any true association with category,
to see how the integration outputs change.

```{r}
fit <- exper_splsda(icu)
plotIndiv(fit)
```

Question: Do you think weak disease associations in one assay influence the
dimensionality reduction outputs in another? If so, how?

```{r}
null_simulator <- simulator
null_simulator[[1]] <- simulator[[1]] |>
  scDesigner::mutate(1:180, link = ~ 1, family = ~ GaussianLSS()) |>
  estimate(nu = 0.05)

# null_simulator[[2]] <- simulator[[2]] |>
#   scDesigner::mutate(1:18, link = ~ 1, family = ~ GaussianLSS()) |>
#   estimate(nu = 0.05)
```

```{r}
icu_sim <- map(null_simulator, sample)
fit <- exper_splsda(icu_sim)
plotIndiv(fit)
```

<button onclick="myFunction(&#39;q1&#39;)">
Show solution
</button>
::: {#q-spls style="display:none"}
:::